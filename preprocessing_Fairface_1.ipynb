{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhowe23/EECE-5644-Final-Project/blob/main/preprocessing_Fairface_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8f7ee44",
      "metadata": {
        "id": "e8f7ee44"
      },
      "source": [
        "# FairFace Preprocessing Pipeline for Bias Mitigation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "462989a3",
      "metadata": {
        "id": "462989a3"
      },
      "source": [
        "# Block 1: Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cfafd0c",
      "metadata": {
        "id": "7cfafd0c"
      },
      "outputs": [],
      "source": [
        "from datasets import load_from_disk\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, MaxAbsScaler, RobustScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold,train_test_split\n",
        "from collections import Counter\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from sklearn import preprocessing\n",
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pprint\n",
        "from collections import defaultdict\n",
        "from tkinter import font\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from aif360.datasets import StandardDataset\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fba1c0e",
      "metadata": {
        "id": "5fba1c0e"
      },
      "source": [
        "# Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d14f5f9d",
      "metadata": {
        "id": "d14f5f9d"
      },
      "outputs": [],
      "source": [
        "def dataset_to_feature_matrix(dataset, max_samples=None):\n",
        "    \"\"\"\n",
        "    Convert a HF dataset with an 'image_features' column and 'race' label\n",
        "    into:\n",
        "    X: numpy array of shape (n, d_feat)\n",
        "    y_str: list of race labels (strings)\n",
        "    \"\"\"\n",
        "    if max_samples is None:\n",
        "        n = len(dataset)\n",
        "    else:\n",
        "        n = min(len(dataset), max_samples)\n",
        "\n",
        "    #peek at first example to get feature dimensionality\n",
        "    d_feat = len(dataset[0][\"image_features\"])\n",
        "    X = np.zeros((n, d_feat), dtype=np.float32)\n",
        "    y_str = []\n",
        "\n",
        "    for i in range(n):\n",
        "        ex = dataset[i]\n",
        "        feats = np.array(ex[\"image_features\"], dtype=np.float32)\n",
        "        X[i] = feats\n",
        "        y_str.append(ex[\"race\"])\n",
        "\n",
        "    return X, y_str"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34c9f3d0",
      "metadata": {
        "id": "34c9f3d0"
      },
      "source": [
        "# Block 2 : Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ddf4c79",
      "metadata": {
        "id": "4ddf4c79"
      },
      "outputs": [],
      "source": [
        "# load datasets\n",
        "\n",
        "#if testing with 70/30 split:\n",
        "#TRAIN_PATH = \"biased_train_70_30_split\"\n",
        "#train_ds = load_from_disk(TRAIN_PATH)\n",
        "\n",
        "#TEST_PATH = \"balanced_test_70_30_split\"\n",
        "#test_ds = load_from_disk(TEST_PATH)\n",
        "\n",
        "\n",
        "#if testing with 90/10 split:\n",
        "TRAIN_PATH = \"biased_train\"\n",
        "train_ds = load_from_disk(TRAIN_PATH)\n",
        "\n",
        "TEST_PATH = \"balanced_test\"\n",
        "test_ds = load_from_disk(TEST_PATH)\n",
        "\n",
        "print(\"Biased train size:\", len(train_ds))\n",
        "print(\"Balanced test size:\", len(test_ds))\n",
        "print(\"Train race counts:\", Counter(train_ds[\"race\"]))\n",
        "print(\"Test race counts:\", Counter(test_ds[\"race\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02980a8a",
      "metadata": {
        "id": "02980a8a"
      },
      "source": [
        "# Block 3: Building Feature Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d1e483",
      "metadata": {
        "id": "27d1e483"
      },
      "outputs": [],
      "source": [
        "MAX_TRAIN = None  #or cap to eg. 12000\n",
        "\n",
        "print(\"\\nConverting train set to feature matrix...\")\n",
        "X_train, y_train_str = dataset_to_feature_matrix(train_ds, max_samples=MAX_TRAIN)\n",
        "print(\"Converting test set to feature matrix...\")\n",
        "X_test, y_test_str = dataset_to_feature_matrix(test_ds)\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test  shape:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd86e142",
      "metadata": {
        "id": "bd86e142"
      },
      "source": [
        "# Block 4: Encoding Race Labels as Integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2d62346",
      "metadata": {
        "id": "a2d62346"
      },
      "outputs": [],
      "source": [
        "all_races = sorted(set(y_train_str) | set(y_test_str))\n",
        "race_to_id = {r: i for i, r in enumerate(all_races)}\n",
        "id_to_race = {i: r for r, i in race_to_id.items()}\n",
        "\n",
        "y_train = np.array([race_to_id[r] for r in y_train_str], dtype=int)\n",
        "y_test = np.array([race_to_id[r] for r in y_test_str], dtype=int)\n",
        "\n",
        "print(\"Races (classes):\", all_races)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03cd6c9b",
      "metadata": {
        "id": "03cd6c9b"
      },
      "source": [
        "# Block 5: Different Scaling Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41495ff",
      "metadata": {
        "id": "b41495ff"
      },
      "outputs": [],
      "source": [
        "scaling_methods = {\n",
        "    'StandardScaler': StandardScaler(),\n",
        "    'MinMaxScaler': MinMaxScaler(),\n",
        "    #'RobustScaler': RobustScaler(),\n",
        "    #'Normalizer': Normalizer(),\n",
        "    'MaxAbsScaler': MaxAbsScaler()\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc535c9",
      "metadata": {
        "id": "4dc535c9"
      },
      "source": [
        "# Block 6: Sampling Strategies to Help Fix Imbalances"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dae67de",
      "metadata": {
        "id": "6dae67de"
      },
      "source": [
        "### 6.1 Random UnderSampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44e06fc",
      "metadata": {
        "id": "c44e06fc"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"\\nUndersampling the majority class to fix imbalance...\")\n",
        "rand_under_sampling = RandomUnderSampler(random_state= 42)\n",
        "X_train_rand_under, y_train_rand_under = rand_under_sampling.fit_resample(X_train,y_train)\n",
        "print(\"Train set size with undersampling:\", X_train_rand_under.shape)\n",
        "\n",
        "fix_format = {str(k): v for k, v in Counter(y_train_rand_under).items()}\n",
        "print(\"Counts for train set class with undersampling:\")\n",
        "pprint.pprint(fix_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4408d0a",
      "metadata": {
        "id": "f4408d0a"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0e45e8d7",
      "metadata": {
        "id": "0e45e8d7"
      },
      "source": [
        "### 6.2 Random Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "270d8ad4",
      "metadata": {
        "id": "270d8ad4"
      },
      "outputs": [],
      "source": [
        "print(\"\\nOversampling the majority class to fix imbalance...\")\n",
        "rand_over_sampling = RandomOverSampler(random_state= 42)\n",
        "X_train_rand_over, y_train_rand_over = rand_over_sampling.fit_resample(X_train,y_train)\n",
        "print(\"Train set size with oversampling:\", X_train_rand_over.shape)\n",
        "\n",
        "fix_format = {str(k): v for k, v in Counter(y_train_rand_over).items()}\n",
        "print(\"Counts for train set class with oversampling:\")\n",
        "pprint.pprint(fix_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac4e300",
      "metadata": {
        "id": "bac4e300"
      },
      "source": [
        "### 6.3 SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "817e0efc",
      "metadata": {
        "id": "817e0efc"
      },
      "outputs": [],
      "source": [
        "print(\"\\nSMOTE Sampling to fix imbalance...\")\n",
        "smote_sampler = SMOTE(sampling_strategy= 'auto',random_state=42,k_neighbors=5)\n",
        "#smote_sampler = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote= smote_sampler.fit_resample(X_train, y_train)\n",
        "print(\"SMOTE sampled train set size:\", X_train_smote.shape)\n",
        "\n",
        "fix_format = {str(k): v for k, v in Counter(y_train_smote).items()}\n",
        "print(\"Counts for SMOTE sampled train set class:\")\n",
        "pprint.pprint(fix_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85a277aa",
      "metadata": {
        "id": "85a277aa"
      },
      "source": [
        "### 6.4 All Sampling Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "590c0e74",
      "metadata": {
        "id": "590c0e74"
      },
      "outputs": [],
      "source": [
        "sampling_methods = {\"No Sampling\": None,\n",
        "                    \"Undersampling\": RandomUnderSampler(random_state=42),\n",
        "                    \"Oversampling\": RandomOverSampler(random_state=42),\n",
        "                    \"SMOTE\": SMOTE(random_state=42)}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eca7725",
      "metadata": {
        "id": "6eca7725"
      },
      "source": [
        "# Block 7: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae1bd3c6",
      "metadata": {
        "id": "ae1bd3c6"
      },
      "outputs": [],
      "source": [
        "print(\"\\nTraining multinomial Logistic Regression on image_features...\")\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\",\n",
        "    n_jobs=-1,\n",
        "    #balancing weight\n",
        "    #class_weight=\"balanced\",\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d35829fe",
      "metadata": {
        "id": "d35829fe"
      },
      "outputs": [],
      "source": [
        "print(\"Fitting Logistic Regression...\")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "\n",
        "print(\"\\nEvaluating on balanced test set...\")\n",
        "y_pred = clf.predict(X_test)\n",
        "overall_acc = accuracy_score(y_test, y_pred)\n",
        "print(\"\\nOverall test accuracy: {:.3f}\".format(overall_acc))\n",
        "\n",
        "print(\"\\nAccuracy by race:\")\n",
        "for race in all_races:\n",
        "    idx = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "    if len(idx) == 0:\n",
        "        continue\n",
        "    acc_r = accuracy_score(y_test[idx], y_pred[idx])\n",
        "    print(\"  {:20s}  n={:4d}  acc={:.3f}\".format(race, len(idx), acc_r))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "026d2752",
      "metadata": {
        "id": "026d2752"
      },
      "source": [
        "# Block 8:  Pipeline Experiments\n",
        "\n",
        "#####  1. Logistic Regression with different scaling methods.\n",
        "#####  2. Logistic Regression with different scaling and sampling methods.\n",
        "#####  3. Logistic Regression with different scaling and sampling methods using GridSearch (cv).\n",
        "#####  4. Logistic Regression with balanced weights and different scaling methods.\n",
        "#####  5. Logistic Regression with balanced weights and different scaling methods using GridSearch (cv).\n",
        "#####  6. Logistic Regression and Reweighing (AIF360)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3165a8eb",
      "metadata": {
        "id": "3165a8eb"
      },
      "source": [
        "## 8.1 Logistic Regression with different scaling methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "632c0f97",
      "metadata": {
        "id": "632c0f97"
      },
      "outputs": [],
      "source": [
        "#logistic regression with just scaling\n",
        "\n",
        "acc_results_sc = {}\n",
        "race_wise_acc_sc = {}\n",
        "race_wise_std_sc = {}\n",
        "fairness_metrics_race = {}\n",
        "\n",
        "best_fairness = float('inf')\n",
        "best_model_fairness = None\n",
        "\n",
        "\n",
        "print(\"\\nLogistic Regression with different scaling methods:\")\n",
        "for scaler_type, scaler in scaling_methods.items():\n",
        "    pipeline = ImbPipeline([('scaler',scaler),('classifier', clf)])\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    overall_acc_sc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nOverall test accuracy with {scaler_type}: {overall_acc_sc:.3f}\")\n",
        "    print(\"\\nAccuracy by race:\")\n",
        "    acc_results_sc[(scaler_type)] = overall_acc_sc\n",
        "\n",
        "    for race in all_races:\n",
        "        idx = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "        acc_r = accuracy_score(y_test[idx], y_pred[idx])\n",
        "        acc_results_sc[(scaler_type, race)] = acc_r\n",
        "        race_wise_acc_sc[(scaler_type, race)] = acc_r\n",
        "        race_wise_std_sc[(scaler_type, race)] = np.std(y_pred[idx])\n",
        "        std_acc = (y_pred[idx] == y_test[idx]).astype(int)\n",
        "        std_r = np.std(std_acc)\n",
        "        race_wise_acc_sc[(scaler_type, race)] = acc_r\n",
        "        race_wise_std_sc[(scaler_type, race)] = std_r\n",
        "        print(f\"  {race:20s}  n={len(idx):4d}  acc={acc_r:.3f}  std={std_r:.3f}\")\n",
        "\n",
        "    # tracking fairness sdp and disparate impact per race against White\n",
        "    idx_race_White = [i for i, r in enumerate(y_test_str) if r == 'White']\n",
        "    y_pred_race_White = y_pred[idx_race_White]\n",
        "    abs_eod = []\n",
        "    abs_spd = []\n",
        "    #abs_di = []\n",
        "\n",
        "    print(f\"\\n\")\n",
        "    print(f\"Fairness metrics for each race compared to White:\")\n",
        "    for race in all_races:\n",
        "        if race == 'White':\n",
        "            continue\n",
        "        idx_race = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "        y_pred_race = y_pred[idx_race]\n",
        "\n",
        "\n",
        "        #equal opportunity difference\n",
        "        priv_groups_TPR = np.sum((y_test[idx_race_White] == race_to_id['White']) & (y_pred[idx_race_White] == race_to_id['White'])) / (np.sum(y_test[idx_race_White] == race_to_id['White']))\n",
        "        unpriv_groups_TPR = np.sum((y_test[idx_race] == race_to_id[race]) & (y_pred[idx_race] == race_to_id[race])) / (np.sum(y_test[idx_race] == race_to_id[race]))\n",
        "\n",
        "        eod = priv_groups_TPR - unpriv_groups_TPR\n",
        "\n",
        "\n",
        "        #statistical parity difference\n",
        "        static_parity_diff = y_pred_race.mean() - y_pred_race_White.mean()\n",
        "\n",
        "        #disparate impact\n",
        "        #disparate_impact = y_pred_race.mean() / (y_pred_race_White.mean() + 1e-10)\n",
        "\n",
        "\n",
        "        fairness_metrics_race[(scaler_type, race)] = {\n",
        "            'statistical_parity_difference': static_parity_diff,\n",
        "            #'disparate_impact': disparate_impact,\n",
        "            'equal_opportunity_difference': eod\n",
        "        }\n",
        "\n",
        "\n",
        "        #print(f\"  {race:20s}  statistical_parity_difference: {static_parity_diff:.3f}  disparate_impact: {disparate_impact:.3f}, equal_opportunity_difference: {eod:.3f}\")\n",
        "        print(f\"  {race:20s}  statistical_parity_difference: {static_parity_diff:.3f}  equal_opportunity_difference: {eod:.3f}\" )\n",
        "        abs_spd.append(abs(static_parity_diff))\n",
        "        #abs_di.append(abs(disparate_impact - 1))\n",
        "        abs_eod.append(abs(eod))\n",
        "\n",
        "    #overall fairness for each scaler type not all races not per races\n",
        "\n",
        "    abs_eod = np.mean(abs_eod)\n",
        "    abs_spd = np.mean(abs_spd)\n",
        "    #abs_di = np.mean(abs_di)\n",
        "\n",
        "    #fairness = abs_spd + abs_di + abs_eod\n",
        "    fairness = abs_spd + abs_eod\n",
        "    if fairness < best_fairness:\n",
        "        best_fairness = fairness\n",
        "        best_model_fairness = (scaler_type)\n",
        "\n",
        "    print()\n",
        "    print(f\"{scaler_type} -> overall fairness: {fairness:.3f}\")\n",
        "    # print(f\"{scaler_type} -> overall SDP: {abs_spd:.3f}, overall DI: {abs_di:.3f}, overall EOD: {abs_eod:.3f}\")\n",
        "    print (f\"{scaler_type} -> overall SDP: {abs_spd:.3f}, overall EOD: {abs_eod:.3f}\")\n",
        "    print()\n",
        "print(f\"Model with best fairness based on SPD (statistical parity difference) and EOD (equal opportunity difference): {best_model_fairness} with fairness: {best_fairness:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ea2fa4d",
      "metadata": {
        "id": "5ea2fa4d"
      },
      "source": [
        "## 8.2 Logistic Regression using different scaling and sampling methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b4ee279",
      "metadata": {
        "id": "2b4ee279"
      },
      "outputs": [],
      "source": [
        "acc_results_sc_samp1 = {}\n",
        "race_wise_acc_sc_samp1 = {}\n",
        "race_wise_std_sc_samp1 = {}\n",
        "fairness_metrics_race = {}\n",
        "\n",
        "best_fairness = float('inf')\n",
        "best_model_fairness = None\n",
        "\n",
        "# Pipeline elements for each combination of scaler and sampler, considering PCA\n",
        "for sampler_type, sampler in sampling_methods.items():\n",
        "    for scaler_type, scaler in scaling_methods.items():\n",
        "        if sampler_type != \"No Sampling\":\n",
        "            pipeline = ImbPipeline(\n",
        "            [('scaler', scaler),\n",
        "            ('pca', PCA(n_components=0.9, random_state=42)),\n",
        "            ('sampler', sampler),\n",
        "            ('logisticregression', clf)]\n",
        "            )\n",
        "        else:\n",
        "            pipeline = ImbPipeline(\n",
        "            [('scaler', scaler),\n",
        "            ('pca', PCA(n_components=0.9, random_state=42)),\n",
        "            ('sampler', 'passthrough'),\n",
        "            ('logisticregression', clf)]\n",
        "            )\n",
        "        print(f\"\\nApplying {scaler_type} and {sampler_type}...\" )\n",
        "\n",
        "        # Fit the pipeline\n",
        "        pipeline.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "        #Evaluate\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        overall_acc_sc_samp = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\n Sampling Method: {} | Scaling Method: {} | Overall test accuracy: {:.3f}\".format(sampler_type, scaler_type, overall_acc_sc_samp))\n",
        "        #results\n",
        "        acc_results_sc_samp1[(scaler_type, sampler_type)] = overall_acc_sc_samp\n",
        "\n",
        "        for race in all_races:\n",
        "            idx = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "            if len(idx) == 0:\n",
        "                continue\n",
        "            acc_r = accuracy_score(y_test[idx], y_pred[idx])\n",
        "            std_acc = (y_pred[idx] == y_test[idx]).astype(int)\n",
        "            std_r = np.std(std_acc)\n",
        "            print(f\"  {race:20s}  n={len(idx):4d}  acc={acc_r:.3f}  std={std_r:.3f}\")\n",
        "            race_wise_acc_sc_samp1[(scaler_type, sampler_type, race)] = acc_r\n",
        "            race_wise_std_sc_samp1[(scaler_type, sampler_type, race)] = std_r\n",
        "\n",
        "    # tracking fairness sdp and disparate impact per race against Whitef\n",
        "        idx_race_White = [i for i, r in enumerate(y_test_str) if r == 'White']\n",
        "        y_pred_race_White = y_pred[idx_race_White]\n",
        "        abs_spd = []\n",
        "        abs_eod = []\n",
        "       # abs_di = []\n",
        "\n",
        "\n",
        "        print(f\"\\n\")\n",
        "        print(f\"Fairness metrics for each race compared to White:\")\n",
        "\n",
        "        for race in all_races:\n",
        "            if race == 'White':\n",
        "                continue\n",
        "            idx_race = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "            y_pred_race = y_pred[idx_race]\n",
        "\n",
        "            #equal opportunity difference\n",
        "            priv_groups_TPR = np.sum((y_test[idx_race_White] == race_to_id['White']) & (y_pred[idx_race_White] == race_to_id['White'])) / (np.sum(y_test[idx_race_White] == race_to_id['White']))\n",
        "            unpriv_groups_TPR = np.sum((y_test[idx_race] == race_to_id[race]) & (y_pred[idx_race] == race_to_id[race])) / (np.sum(y_test[idx_race] == race_to_id[race]))\n",
        "\n",
        "            eod = priv_groups_TPR - unpriv_groups_TPR\n",
        "\n",
        "            #statistical parity difference\n",
        "            static_parity_diff = y_pred_race.mean() - y_pred_race_White.mean()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            fairness_metrics_race[(scaler_type, race)] = {\n",
        "                'statistical_parity_difference': static_parity_diff,\n",
        "                'equal_opportunity_difference': eod\n",
        "            }\n",
        "            print(f\"{race:20s} statistical parity_difference: {static_parity_diff:.3f},  equal_opportunity_difference: {eod:.3f}\")\n",
        "\n",
        "            abs_spd.append(abs(static_parity_diff))\n",
        "            abs_eod.append(abs(eod))\n",
        "\n",
        "        abs_spd = np.mean(abs_spd)\n",
        "        abs_eod = np.mean(abs_eod)\n",
        "\n",
        "        fairness = abs_spd + abs_eod\n",
        "\n",
        "        if fairness < best_fairness:\n",
        "            best_fairness = fairness\n",
        "            best_model_fairness = (scaler_type, sampler_type)\n",
        "        print()\n",
        "        print(f\"{scaler_type},{sampler_type} -> overall_fairness: {fairness:.3f}\")\n",
        "        print(f\"{scaler_type},{sampler_type} -> overall SDP: {abs_spd:.3f}, overall EOD: {abs_eod:.3f}\")\n",
        "\n",
        "    print()\n",
        "print(f\"Model with best fairness based on SPD (statistical parity difference) and EOD (equal opportunity difference): {best_model_fairness} with fairness: {best_fairness:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61fc796a",
      "metadata": {
        "id": "61fc796a"
      },
      "source": [
        "## 8.3 Logistic Regression using different scaling and sampling methods + Grid Search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "831e13b1",
      "metadata": {
        "id": "831e13b1"
      },
      "outputs": [],
      "source": [
        "acc_results_sc_samp = {}\n",
        "race_wise_acc_sc_samp = {}\n",
        "race_wise_std_sc_samp = {}\n",
        "fairness_metrics_race = {}\n",
        "\n",
        "best_fairness = float('inf')\n",
        "best_model_fairness = None\n",
        "\n",
        "# Pipeline elements for each combination of scaler and sampler, considering PCA\n",
        "for sampler_type, sampler in sampling_methods.items():\n",
        "    for scaler_type, scaler in scaling_methods.items():\n",
        "        if sampler_type != \"No Sampling\":\n",
        "            pipeline = ImbPipeline(\n",
        "            [('scaler', scaler),\n",
        "            ('pca', PCA(n_components=0.9, random_state=42)),\n",
        "            ('sampler', sampler),\n",
        "            ('logisticregression', clf)]\n",
        "            )\n",
        "        else:\n",
        "            pipeline = ImbPipeline(\n",
        "            [('scaler', scaler),\n",
        "            ('pca', PCA(n_components=0.9, random_state=42)),\n",
        "            ('sampler', 'passthrough'),\n",
        "            ('logisticregression', clf)]\n",
        "            )\n",
        "        print(f\"\\nApplying {scaler_type} and {sampler_type}...\" )\n",
        "\n",
        "        # GridSearchCV for hyperparameter tuning\n",
        "        grid_search = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid={\n",
        "                'logisticregression__C': [0.1, 1, 10],\n",
        "                'logisticregression__penalty': ['l2'],\n",
        "            },\n",
        "            scoring='accuracy',\n",
        "            cv=StratifiedKFold(n_splits=3),\n",
        "            n_jobs=-1,\n",
        "            verbose=1\n",
        "        )\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "        #Evaluate\n",
        "        y_pred = grid_search.predict(X_test)\n",
        "        overall_acc_sc_samp = accuracy_score(y_test, y_pred)\n",
        "        print(\"\\n Sampling Method: {} | Scaling Method: {} | Overall test accuracy: {:.3f}\".format(sampler_type, scaler_type, overall_acc_sc_samp))\n",
        "        #results\n",
        "        acc_results_sc_samp[(scaler_type, sampler_type)] = overall_acc_sc_samp\n",
        "\n",
        "        for race in all_races:\n",
        "            idx = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "            if len(idx) == 0:\n",
        "                continue\n",
        "            acc_r = accuracy_score(y_test[idx], y_pred[idx])\n",
        "            std_acc = (y_pred[idx] == y_test[idx]).astype(int)\n",
        "            std_r = np.std(std_acc)\n",
        "            print(f\"  {race:20s}  n={len(idx):4d}  acc={acc_r:.3f}  std={std_r:.3f}\")\n",
        "            race_wise_acc_sc_samp[(scaler_type, sampler_type, race)] = acc_r\n",
        "            race_wise_std_sc_samp[(scaler_type, sampler_type, race)] = std_r\n",
        "\n",
        "        # tracking fairness sdp and disparate impact per race against White\n",
        "        idx_race_White = [i for i, r in enumerate(y_test_str) if r == 'White']\n",
        "        y_pred_race_White = y_pred[idx_race_White]\n",
        "        abs_eod = []\n",
        "        abs_spd = []\n",
        "\n",
        "\n",
        "        print(f\"\\n\")\n",
        "        print(f\"Fairness metrics for each race compared to White:\")\n",
        "\n",
        "        for race in all_races:\n",
        "            if race == 'White':\n",
        "                continue\n",
        "            idx_race = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "            y_pred_race = y_pred[idx_race]\n",
        "\n",
        "            #equal opportunity difference\n",
        "            priv_groups_TPR = np.sum((y_test[idx_race_White] == race_to_id['White']) & (y_pred[idx_race_White] == race_to_id['White'])) / (np.sum(y_test[idx_race_White] == race_to_id['White']))\n",
        "            unpriv_groups_TPR = np.sum((y_test[idx_race] == race_to_id[race]) & (y_pred[idx_race] == race_to_id[race])) / (np.sum(y_test[idx_race] == race_to_id[race]))\n",
        "\n",
        "            eod = priv_groups_TPR - unpriv_groups_TPR\n",
        "\n",
        "            #statistical parity difference\n",
        "            static_parity_diff = y_pred_race.mean() - y_pred_race_White.mean()\n",
        "\n",
        "            fairness_metrics_race[(scaler_type, race)] = {\n",
        "                'statistical_parity_difference': static_parity_diff,\n",
        "                'equal_opportunity_difference': eod\n",
        "            }\n",
        "            print(f\"  {race:20s}  statistical_parity_difference: {static_parity_diff:.3f}  equal_opportunity_difference: {eod:.3f}\")\n",
        "\n",
        "            abs_spd.append(abs(static_parity_diff))\n",
        "            abs_eod.append(abs(eod))\n",
        "        abs_spd = np.mean(abs_spd)\n",
        "        abs_eod = np.mean(abs_eod)\n",
        "\n",
        "        fairness = abs_spd + abs_eod\n",
        "\n",
        "        if fairness < best_fairness:\n",
        "            best_fairness = fairness\n",
        "            best_model_fairness = (scaler_type, sampler_type)\n",
        "        print()\n",
        "        print(f\"{scaler_type},{sampler_type} -> overall fairness: {fairness:.3f}\")\n",
        "        print(f\"{scaler_type},{sampler_type} -> overall SDP: {abs_spd:.3f}, overall EOD: {abs_eod:.3f}\")\n",
        "\n",
        "    print()\n",
        "print(f\"Model with best fairness based on SPD (statistical parity difference) and EOD (equal opportunity difference): {best_model_fairness} with fairness: {best_fairness:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae9d77a",
      "metadata": {
        "id": "3ae9d77a"
      },
      "source": [
        "## 8.4 Logistic Regression with balanced weights and different scaling methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d0727e9",
      "metadata": {
        "id": "8d0727e9"
      },
      "outputs": [],
      "source": [
        "# with class weights = balanced and no sampling\n",
        "clf_lr = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\",\n",
        "    n_jobs=-1,\n",
        "    #balancing weight\n",
        "    class_weight=\"balanced\",\n",
        "    verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dd1c3f9",
      "metadata": {
        "id": "4dd1c3f9"
      },
      "outputs": [],
      "source": [
        "\n",
        "#No sampling methods, using class weight = balanced\n",
        "\n",
        "acc_results_bw_sc1 = {}\n",
        "race_wise_acc_bw_sc1 = {}\n",
        "race_wise_std_bw_sc1 = {}\n",
        "fairness_metrics_race = {}\n",
        "# Pipeline elements for each combination of scaler, considering PCA\n",
        "\n",
        "\n",
        "best_fairness = float('inf')\n",
        "best_model_fairness = None\n",
        "\n",
        "for scaler_type, scaler in scaling_methods.items():\n",
        "    pipeline = ImbPipeline(\n",
        "        [('scaler', scaler),\n",
        "        ('pca', PCA(n_components=0.9, random_state=42)),\n",
        "        ('logisticregression', clf_lr)]\n",
        "    )\n",
        "    print(f\"\\nApplying {scaler_type}...\")\n",
        "\n",
        "    #Grid Search CV for pipeline\n",
        "    grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid={'logisticregression__C': [0.1, 1, 10],'logisticregression__penalty': ['l2']},\n",
        "    scoring='accuracy',\n",
        "    #cv=StratifiedKFold(n_splits=5),\n",
        "    cv=StratifiedKFold(n_splits=3),\n",
        "    n_jobs=1,\n",
        "    verbose=0\n",
        "    )\n",
        "\n",
        "    # Evaluate\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "    overall_acc_bw_sc = accuracy_score(y_test, y_pred)\n",
        "    print(\"\\n Balanced weight -> Scaling Method: {} | Overall test accuracy: {:.3f}\".format(scaler_type, overall_acc_bw_sc))\n",
        "\n",
        "    acc_results_bw_sc1[(scaler_type)] = overall_acc_bw_sc\n",
        "    for race in all_races:\n",
        "        idx = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "        acc_r = accuracy_score(y_test[idx], y_pred[idx])\n",
        "        std_acc = (y_pred[idx] == y_test[idx]).astype(int)\n",
        "        std_r = np.std(std_acc)\n",
        "        print(f\" {race:20s}  n={len(idx):4d}  acc={acc_r:.3f}  std={std_r:.3f}\")\n",
        "        race_wise_acc_bw_sc1[(scaler_type, race)] = acc_r\n",
        "        race_wise_std_bw_sc1[(scaler_type, race)] = std_r\n",
        "\n",
        "    # tracking fairness sdp and disparate impact per race against White\n",
        "    idx_race_White = [i for i, r in enumerate(y_test_str) if r == 'White']\n",
        "    y_pred_race_White = y_pred[idx_race_White]\n",
        "    abs_eod = []\n",
        "    abs_spd = []\n",
        "\n",
        "    print(f\"\\n\")\n",
        "    print(f\"Fairness metrics for each race compared to White:\")\n",
        "\n",
        "    for race in all_races:\n",
        "        if race == 'White':\n",
        "            continue\n",
        "        idx_race = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "        y_pred_race = y_pred[idx_race]\n",
        "\n",
        "        #equal opportunity difference\n",
        "        priv_groups_TPR = np.sum((y_test[idx_race_White] == race_to_id['White']) & (y_pred[idx_race_White] == race_to_id['White'])) / (np.sum(y_test[idx_race_White] == race_to_id['White']))\n",
        "        unpriv_groups_TPR = np.sum((y_test[idx_race] == race_to_id[race]) & (y_pred[idx_race] == race_to_id[race])) / (np.sum(y_test[idx_race] == race_to_id[race]))\n",
        "\n",
        "        eod = priv_groups_TPR - unpriv_groups_TPR\n",
        "\n",
        "        #statistical parity difference\n",
        "        static_parity_diff = y_pred_race.mean() - y_pred_race_White.mean()\n",
        "\n",
        "\n",
        "        fairness_metrics_race[(scaler_type, race)] = {\n",
        "                'statistical_parity_difference': static_parity_diff,\n",
        "                'equal_opportunity_difference': eod\n",
        "            }\n",
        "        print(f\"  {race:20s}  statistical_parity_difference: {static_parity_diff:.3f}  equal_opportunity_difference: {eod:.3f}\")\n",
        "\n",
        "        abs_spd.append(abs(static_parity_diff))\n",
        "        abs_eod.append(abs(eod))\n",
        "    abs_spd = np.mean(abs_spd)\n",
        "    abs_eod = np.mean(abs_eod)\n",
        "\n",
        "    fairness = abs_spd + abs_eod\n",
        "    if fairness < best_fairness:\n",
        "            best_fairness = fairness\n",
        "            best_model_fairness = (scaler_type)\n",
        "    print()\n",
        "    print(f\"{scaler_type} -> overall fairness: {fairness:.3f}\")\n",
        "    print(f\"{scaler_type} -> overall SDP: {abs_spd:.3f}, overall EOD: {abs_eod:.3f}\")\n",
        "print()\n",
        "print(f\"Model with best fairness based on SPD (statistical parity difference) and EOD (equal opportunity difference): {best_model_fairness} with fairness: {best_fairness:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "118b8707",
      "metadata": {
        "id": "118b8707"
      },
      "source": [
        "## 8.5 Logistic Regression with balanced weights and different scaling methods + GridSearch (CV)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531dda2c",
      "metadata": {
        "id": "531dda2c"
      },
      "outputs": [],
      "source": [
        "\n",
        "acc_results_bw_sc = {}\n",
        "race_wise_acc_bw_sc = {}\n",
        "race_wise_std_bw_sc = {}\n",
        "fairness_metrics_race = {}\n",
        "\n",
        "best_fairness = float('inf')\n",
        "best_model_fairness = None\n",
        "# Pipeline elements for each combination of scaler, considering PCA\n",
        "\n",
        "for scaler_type, scaler in scaling_methods.items():\n",
        "    pipeline = ImbPipeline(\n",
        "        [('scaler', scaler),\n",
        "        ('pca', PCA(n_components=0.9, random_state=42)),\n",
        "        ('logisticregression', clf_lr)]\n",
        "        )\n",
        "    print(f\"\\nApplying {scaler_type}...\" )\n",
        "\n",
        "    #Grid Search CV for pipeline\n",
        "    grid_search = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid={'logisticregression__C': [0.1, 1, 10],'logisticregression__penalty': ['l2']},\n",
        "            scoring='accuracy',\n",
        "            #cv=StratifiedKFold(n_splits=5),\n",
        "            cv=StratifiedKFold(n_splits=3),\n",
        "            n_jobs=1,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "        #Evaluate\n",
        "    y_pred = grid_search.predict(X_test)\n",
        "    overall_acc = accuracy_score(y_test, y_pred)\n",
        "    print(\"\\nBalanced weight -> Scaling Method: {}\".format(scaler_type))\n",
        "    print(\"Overall test accuracy: {:>2.3f}\".format(overall_acc))\n",
        "        #results\n",
        "    acc_results_bw_sc[(scaler_type)] = overall_acc\n",
        "\n",
        "    for race in all_races:\n",
        "        idx = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "        acc_r = accuracy_score(y_test[idx], y_pred[idx])\n",
        "        std_acc = (y_pred[idx] == y_test[idx]).astype(int)\n",
        "        std_r = np.std(std_acc)\n",
        "        #mean difference\n",
        "        print(f\"{race:20s}  n={len(idx):4d}  acc={acc_r:>2.3f}  std={std_r:>2.3f}\")\n",
        "        race_wise_std_bw_sc[(scaler_type, race)] = std_r\n",
        "\n",
        "    # tracking fairness sdp and disparate impact per race against White\n",
        "    idx_race_White = [i for i, r in enumerate(y_test_str) if r == 'White']\n",
        "    y_pred_race_White = y_pred[idx_race_White]\n",
        "    abs_eod = []\n",
        "    abs_spd = []\n",
        "\n",
        "    print(f\"\\n\")\n",
        "    print(f\"Fairness metrics for each race compared to White:\")\n",
        "\n",
        "    for race in all_races:\n",
        "        if race == 'White':\n",
        "            continue\n",
        "        idx_race = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "        y_pred_race = y_pred[idx_race]\n",
        "\n",
        "        #equal opportunity difference\n",
        "        priv_groups_TPR = np.sum((y_test[idx_race_White] == race_to_id['White']) & (y_pred[idx_race_White] == race_to_id['White'])) / (np.sum(y_test[idx_race_White] == race_to_id['White']))\n",
        "        unpriv_groups_TPR = np.sum((y_test[idx_race] == race_to_id[race]) & (y_pred[idx_race] == race_to_id[race])) / (np.sum(y_test[idx_race] == race_to_id[race]))\n",
        "\n",
        "        eod = priv_groups_TPR - unpriv_groups_TPR\n",
        "\n",
        "        #statistical parity difference\n",
        "        static_parity_diff = y_pred_race.mean() - y_pred_race_White.mean()\n",
        "\n",
        "\n",
        "        fairness_metrics_race[(scaler_type, race)] = {\n",
        "            'statistical_parity_difference': static_parity_diff,\n",
        "            'equal_opportunity_difference': eod\n",
        "                }\n",
        "\n",
        "        print(f\"{race:20s}  statistical_parity_difference: {static_parity_diff:.3f}, equal_opportunity_difference: {eod:.3f}\")\n",
        "\n",
        "        abs_spd.append(abs(static_parity_diff))\n",
        "        abs_eod.append(abs(eod))\n",
        "    abs_spd = np.mean(abs_spd)\n",
        "    abs_eod = np.mean(abs_eod)\n",
        "\n",
        "    fairness = abs_spd + abs_eod\n",
        "    if fairness < best_fairness:\n",
        "        best_fairness = fairness\n",
        "        best_model_fairness = (scaler_type)\n",
        "    print()\n",
        "    print(f\"{scaler_type} -> overall fairness: {fairness:.3f}\")\n",
        "    print(f\"{scaler_type} -> overall SDP: {abs_spd:.3f}, overall EOD: {abs_eod:.3f}\")\n",
        "    print()\n",
        "print(f\"Model with best fairness based on SPD (statistical parity difference) and EOD (equal opportunity difference): {best_model_fairness} with fairness: {best_fairness:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f6ac1f8",
      "metadata": {
        "id": "5f6ac1f8"
      },
      "source": [
        "## 8.6. Logistic Regression and Reweighing (AIF360)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84cb0069",
      "metadata": {
        "id": "84cb0069"
      },
      "outputs": [],
      "source": [
        "train_features = pd.DataFrame(X_train,columns=[f'features_{i}' for i in range(X_train.shape[1])])\n",
        "test_features = pd.DataFrame(X_test,columns=[f'features_{i}' for i in range(X_test.shape[1])])\n",
        "\n",
        "#race is not binary, so mapping helps to create privileged and unprivileged groups\n",
        "race_map = {'Black': 1, 'East Asian': 1, 'Indian': 1, 'Latino_Hispanic': 1, 'Middle Eastern': 1,'Southeast Asian': 1, 'White': 0}\n",
        "\n",
        "'''\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "df_train = preprocessor.fit_transform(df_train)\n",
        "df_test = preprocessor.transform(df_test)\n",
        "'''\n",
        "\n",
        "df_train = train_features.copy()\n",
        "df_train['race'] = y_train_str\n",
        "df_train['labels'] = y_train.astype(float)\n",
        "df_test = test_features.copy()\n",
        "df_test['race'] = y_test_str\n",
        "df_test['labels'] = y_test.astype(float)\n",
        "\n",
        "df_train['race_bin'] = df_train['race'].map(race_map)\n",
        "df_test['race_bin'] = df_test['race'].map(race_map)\n",
        "\n",
        "unprivileged_groups = [{'race_bin': 0.0}]\n",
        "privileged_groups = [{'race_bin': 1.0}]\n",
        "\n",
        "#dropping unrequired columns\n",
        "df_train = df_train.drop(columns=['race'])\n",
        "df_test = df_test.drop(columns=['race'])\n",
        "\n",
        "\n",
        "#AIF360 StandardDataset\n",
        "train_aif = StandardDataset(df_train, label_name='labels', favorable_classes=[1],\n",
        "                            protected_attribute_names=['race_bin'],\n",
        "                            privileged_classes=[[1.0]])\n",
        "\n",
        "test_aif = StandardDataset(df_test, label_name='labels', favorable_classes=[1],\n",
        "                            protected_attribute_names=['race_bin'],\n",
        "                            privileged_classes=[[1.0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88538b36",
      "metadata": {
        "id": "88538b36"
      },
      "outputs": [],
      "source": [
        "#Reweighing\n",
        "RW = Reweighing(unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
        "train_aif_transf = RW.fit_transform(train_aif)\n",
        "test_rw = RW.transform(test_aif)\n",
        "\n",
        "#Extracting features and labels\n",
        "X_train_rw = train_aif_transf.features\n",
        "y_train_rw = train_aif_transf.labels.ravel()\n",
        "\n",
        "'''\n",
        "X_test_aif = test_aif.features\n",
        "y_test_aif = test_aif.labels.ravel()\n",
        "'''\n",
        "\n",
        "X_test_aif = test_rw.features\n",
        "y_test_aif = test_rw.labels.ravel()\n",
        "sample_weights = train_aif_transf.instance_weights\n",
        "\n",
        "print(\"\\nTraining Logistic Regression with Reweighing...\")\n",
        "\n",
        "lr_rw_clf = LogisticRegression(\n",
        "    max_iter=1000,\n",
        "    multi_class=\"multinomial\",\n",
        "    solver=\"lbfgs\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1)\n",
        "\n",
        "rw_acc_results_sc = {}\n",
        "rw_acc_std_sc = {}\n",
        "rw_race_wise_acc = {}\n",
        "fairness_metrics_race = {}\n",
        "\n",
        "\n",
        "for scaler_type, scaler in scaling_methods.items():\n",
        "    pipeline = ImbPipeline([('scaler',scaler),('classifier', lr_rw_clf)])\n",
        "    pipeline.fit(X_train_rw, y_train_rw, classifier__sample_weight=sample_weights)\n",
        "\n",
        "# Evaluate\n",
        "    y_pred = pipeline.predict(X_test_aif)\n",
        "    overall_acc_rw = accuracy_score(y_test_aif, y_pred)\n",
        "    print(f\"\\nOverall test accuracy with Reweighing and {scaler_type}: {overall_acc_rw:.3f}\")\n",
        "    print(\"\\nAccuracy by race:\")\n",
        "    rw_acc_results_sc[(scaler_type)] = overall_acc_rw\n",
        "\n",
        "    for race in all_races:\n",
        "        idx = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "        if len(idx) == 0:\n",
        "            continue\n",
        "        acc_r = accuracy_score(y_test_aif[idx], y_pred[idx])\n",
        "        rw_race_wise_acc[(scaler_type, race)] = acc_r\n",
        "        rw_acc_std_sc[(scaler_type, race)] = np.std(y_pred[idx])\n",
        "        std_acc = (y_pred[idx] == y_test[idx]).astype(int)\n",
        "        std_r = np.std(std_acc)\n",
        "        print(f\"  {race:20s}  n={len(idx):4d}  acc={acc_r:.3f}  std={std_r:.3f}\")\n",
        "\n",
        "    # tracking fairness sdp and disparate impact per race against White\n",
        "\n",
        "    idx_race_white = [i for i, r in enumerate(y_test_str) if r == 'White']\n",
        "    y_pred_race_white = y_pred[idx_race_white]\n",
        "    abs_eod = []\n",
        "    abs_spd = []\n",
        "\n",
        "    print(f\"\\n\")\n",
        "    print(f\"Fairness metrics for each race compared to White:\")\n",
        "\n",
        "    for race in all_races:\n",
        "        if race == 'White':\n",
        "            continue\n",
        "        idx_race = [i for i, r in enumerate(y_test_str) if r == race]\n",
        "        y_pred_race = y_pred[idx_race]\n",
        "\n",
        "        # equal opportunity difference\n",
        "        priv_groups_TPR = np.sum((y_test[idx_race_White] == race_to_id['White']) & (y_pred[idx_race_White] == race_to_id['White'])) / (np.sum(y_test[idx_race_White] == race_to_id['White']))\n",
        "        unpriv_groups_TPR = np.sum((y_test[idx_race] == race_to_id[race]) & (y_pred[idx_race] == race_to_id[race])) / (np.sum(y_test[idx_race] == race_to_id[race]))\n",
        "\n",
        "        eod = priv_groups_TPR - unpriv_groups_TPR\n",
        "\n",
        "        #statistical parity difference\n",
        "        static_parity_diff = y_pred_race.mean() - y_pred_race_white.mean()\n",
        "\n",
        "\n",
        "        fairness_metrics_race[(scaler_type, race)] = {\n",
        "            'statistical_parity_difference': static_parity_diff,\n",
        "            'equal_opportunity_difference': eod\n",
        "        }\n",
        "        print(f\"{race:20s}  statistical_parity_difference: {static_parity_diff:.3f}  equal_opportunity_difference: {eod:.3f}\")\n",
        "\n",
        "\n",
        "        abs_spd.append(abs(static_parity_diff))\n",
        "        abs_eod.append(abs(eod))\n",
        "    abs_spd = np.mean(abs_spd)\n",
        "    abs_eod = np.mean(abs_eod)\n",
        "\n",
        "    best_fairness = abs_spd + abs_eod\n",
        "    print()\n",
        "    print(f\"{scaler_type} -> overall fairness: {best_fairness:.3f}\")\n",
        "    print(f\"{scaler_type} -> overall SPD: {abs_spd:.3f}, overall EOD: {abs_eod:.3f}\")\n",
        "print()\n",
        "print(f\"Model with best fairness with Reweighing based on SPD (statistical parity difference) and EOD (equal opportunity difference): {best_fairness:.3f}, Scaler: {scaler_type}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14b90b0b",
      "metadata": {
        "id": "14b90b0b"
      },
      "source": [
        "## References\n",
        "###### https://huggingface.co/datasets/ryanramos/fairface\n",
        "###### https://codecut.ai/pipeline-gridsearchcv-prevent-data-leakage-when-scaling-the-data-3/#:~:text=May%2023%2C%202023-,Pipeline%20+%20GridSearchCV:%20Prevent%20Data%20Leakage%20when%20Scaling%20the%20Data,previous%20tips%20on%20machine%20learning.\n",
        "######  https://www.geeksforgeeks.org/machine-learning/data-pre-processing-wit-sklearn-using-standard-and-minmax-scaler/\n",
        "###### https://www.geeksforgeeks.org/machine-learning/performing-feature-selection-with-gridsearchcv-in-sklearn/\n",
        "###### https://aif360.readthedocs.io/en/latest/modules/generated/aif360.datasets.StandardDataset.html?utm\n",
        "###### https://medium.com/ibm-data-ai/fairness-in-machine-learning-pre-processing-algorithms-a670c031fba8\n",
        "###### https://chatgpt.com/share/692fec2d-c7c4-800d-a270-93fce02447c6\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (ml_env_313)",
      "language": "python",
      "name": "ml_env_313"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}